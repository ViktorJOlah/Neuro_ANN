{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNE/buN9BXRMig/EEmry+XL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViktorJOlah/Neuro_ANN/blob/main/multicompartmental%20models/L2_3%20PC/final_L2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMydcpkiU-vv",
        "outputId": "230e31cb-5e8e-42a8-acae-1bc94dd6184d"
      },
      "source": [
        "#this is for testing purposes, for getting the most out of GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 25.2 GB  | Proc size: 1.2 GB\n",
            "GPU RAM Free: 15891MB | Used: 389MB | Util   2% | Total 16280MB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgA6eivNXJIF"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd \n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "import pickle\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "from keras import optimizers\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import logging\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import LocallyConnected1D\n",
        "\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipGm88vK3hU"
      },
      "source": [
        "#define basic parameters for fitting\n",
        "\n",
        "params = {\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 1,\n",
        "    \"lr\": 0.0010000,\n",
        "    \"time_steps\": 128,\n",
        "    \"rec_length\": 20000\n",
        "}\n",
        "\n",
        "\n",
        "TIME_STEPS = params[\"time_steps\"]\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "stime = time.time()\n",
        "steps_per_epoch = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wybFFdCqJYH_"
      },
      "source": [
        "#this is an optional step in case NEURON files contain matrix information. in this case, instead of heaving a continous stream of voltages and inputs, every epoch starts with matrix row an column number, which has to be deleted\n",
        "\n",
        "with open('/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/vmi3.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(len(lines))\n",
        "counter = 0\n",
        "for row in lines:\n",
        "    if row[:3] == '200':\n",
        "        lines.pop(counter)\n",
        "    counter += 1\n",
        "\n",
        "print(len(lines))\n",
        "\n",
        "f1 = open('/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/vmi3_1.txt', \"a\")\n",
        "for row in lines:\n",
        "    f1.write(str(row))\n",
        "f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Im_wEaq8JUv"
      },
      "source": [
        "#load final dataset\n",
        "data = pd.read_csv('/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/vmi3_1.txt', sep=\" \", header=None, dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvF14fhIrHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ea276d-009e-44e8-d575-5827065a1bd7"
      },
      "source": [
        "#cleaning a normalizing dataset \n",
        "data.drop([59], axis = 1, inplace=True)\n",
        "data.columns = [str(data.columns[i]) for i in range(59)]\n",
        "\n",
        "for place, val in enumerate(data[\"0\"]):\n",
        "    if val > -20:\n",
        "        data[\"0\"][place] = -20\n",
        "\n",
        "#shift votlages to posiive values     \n",
        "data[\"0\"] = data[\"0\"]+150   #somatic Vm\n",
        "\n",
        "#normalize everything between 0 and 1\n",
        "#the reason for printing these values out, is that if a different dataset is loaded, we have to use the original normalization values\n",
        "print(data[\"0\"].min())\n",
        "print(data[\"0\"].max())\n",
        "data[\"0\"] = (data[\"0\"]-data[\"0\"].min())/data[\"0\"].max()\n",
        "print(data[\"0\"].max())\n",
        "data[\"0\"] = data[\"0\"]/data[\"0\"].max()\n",
        "\n",
        "x_all = np.asarray(data, dtype=\"float32\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48.443\n",
            "130.0\n",
            "0.62736154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tFU4nhuPS4"
      },
      "source": [
        "data.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSkAfrYp-kzp"
      },
      "source": [
        "#reduce dataset for memory conservation\n",
        "x_all = x_all[:4000000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrVp1OT9IwkE"
      },
      "source": [
        "#a basic optional function for selecting traces with high or low firing frequencies\n",
        "too_much_freq = []\n",
        "lower_arr = []\n",
        "\n",
        "for i in range(999):\n",
        "    spikes = 0\n",
        "    for k in range(1000):\n",
        "        if data[\"0\"][i*1001+k] > 0.75 and  data[\"0\"][i*1001+k] > data[\"0\"][i*1001+k-1] and data[\"0\"][i*1001+k] > data[\"0\"][i*1001+k+1]:\n",
        "            spikes += 1\n",
        "    print(spikes)\n",
        "    if spikes > 10:\n",
        "        too_much_freq.append(1)\n",
        "    else:\n",
        "        for k1 in range(1001):\n",
        "            lower_arr.append(i*1001+k1)\n",
        "\n",
        "print(f\"len_too_much_freq {len(too_much_freq)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDhQ2zLFL8iC"
      },
      "source": [
        "\n",
        "del data\n",
        "\n",
        "#this is a different kind of input dataset creation as the arrays wouldn't fit the memory. We need to create generators which will continously generate datasets for fitting\n",
        "#tin order to get a clean and random dataset for fitting a validation, we create random vectors for sample selection \n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "x_all_dim0 = x_all.shape[0]-TIME_STEPS\n",
        "\n",
        "\n",
        "orig_rand = []\n",
        "for place, i in enumerate(lower_arr):\n",
        "    if i%TIME_STEPS > 0.93:\n",
        "        pass\n",
        "    else:\n",
        "        orig_rand.append(i)\n",
        "\n",
        "x_all_dim0 = x_all.shape[0]-TIME_STEPS-10\n",
        "\n",
        "rand_x_train = random.sample(range(x_all_dim0), int(x_all_dim0*0.9))\n",
        "rand_x_test = np.setdiff1d(range(x_all_dim0), rand_x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKqZeIQMMT21"
      },
      "source": [
        "#generator for fitting. It requires a random array in order to avoid creating training batches from continous samples\n",
        "def gen1(df, rand_vec, y_col_index, TIME_STEPS, BATCH_SIZE):\n",
        "    dim_1 = df.shape[1]\n",
        "    x = np.zeros((BATCH_SIZE, TIME_STEPS, dim_1))\n",
        "    y = np.zeros((BATCH_SIZE,1))\n",
        "    try:\n",
        "        place = int(len(rand_vec)*random.random())\n",
        "        randvec = rand_vec[place:place+BATCH_SIZE]\n",
        "        for i in range(BATCH_SIZE):\n",
        "            x[i] = df[randvec[i]:TIME_STEPS+randvec[i]]\n",
        "            y[i] = df[TIME_STEPS+randvec[i], y_col_index:1]\n",
        "    except:\n",
        "        for i in range(BATCH_SIZE):\n",
        "            x[i] = df[:TIME_STEPS]\n",
        "            y[i] = df[TIME_STEPS, y_col_index:1]\n",
        "\n",
        "    yield x, y\n",
        "\n",
        "\n",
        "x_train_counter = tf.data.Dataset.from_generator(gen1, args=[x_all, rand_x_train, 0, TIME_STEPS, BATCH_SIZE], output_types=(tf.float32, tf.float32), output_shapes = ((64,128,59), (64,1)) ).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "x_test_counter = tf.data.Dataset.from_generator(gen1, args=[x_all, rand_x_test, 0, TIME_STEPS, BATCH_SIZE], output_types=(tf.float32, tf.float32), output_shapes = ((64,128,59), (64,1)) ).repeat(count=500).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSrmZuw3gRc6",
        "outputId": "d0ebde96-1383-403a-b6ed-0cab2ce8e445"
      },
      "source": [
        "#CNN-LSTM architecture\n",
        "def create_model_CNN_LSTM2():\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(512, 1, activation='relu', kernel_initializer=\"he_uniform\", padding=\"causal\", kernel_regularizer=regularizers.l1(1e-5), input_shape=(TIME_STEPS, 59)),\n",
        "        #tf.keras.layers.Conv1D(512, 1, activation='relu', kernel_initializer=\"he_uniform\", padding=\"causal\", input_shape=(TIME_STEPS, 95)),\n",
        "        tf.keras.layers.Conv1D(256, 5, activation='relu', kernel_initializer=\"he_uniform\", padding=\"causal\"),\n",
        "        tf.keras.layers.Conv1D(128, 1, activation='tanh', kernel_initializer='glorot_uniform', padding=\"causal\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.RepeatVector(5),\n",
        "        tf.keras.layers.LSTM(128, activation='tanh', kernel_initializer='glorot_uniform', return_sequences=True),\n",
        "        tf.keras.layers.LSTM(128, activation='tanh', kernel_initializer='glorot_uniform', return_sequences=True),\n",
        "        tf.keras.layers.LSTM(128, activation='tanh', kernel_initializer='glorot_uniform', return_sequences=True),\n",
        "        tf.keras.layers.Dense(128, activation='relu', kernel_initializer=\"he_uniform\"),\n",
        "        tf.keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_uniform\"),\n",
        "        tf.keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_uniform\"),\n",
        "        tf.keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_uniform\"),\n",
        "        tf.keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_uniform\"),\n",
        "        tf.keras.layers.Dense(1)])\n",
        "    model.compile(loss=['mae'], optimizer=tf.keras.optimizers.Adamax(learning_rate=0.0001))\n",
        "    return model\n",
        "\n",
        "model = None\n",
        "try:\n",
        "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
        "    print(\"Loaded saved model...\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Model not found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be0AcpcUHa3Y",
        "outputId": "8a1052a8-5523-4b92-af2a-7db635270ca2"
      },
      "source": [
        "#fitting\n",
        "\n",
        "is_update_model = True\n",
        "if model is None or is_update_model:\n",
        "    from keras import backend as K\n",
        "    print(\"Building model...\")\n",
        "    model = create_model_CNN_LSTM2()   #change model checkpoint as well!\n",
        "    model.summary()\n",
        "    #this code below can be used to load halted fitting results, not just the weights, but the optimizaer state as well\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tf.keras.models.load_model('/content/drive/My Drive/V1_DNN/Proof_of_principal/SST/act_weights')\n",
        "        print(\"model loaded\")\n",
        "    except:\n",
        "        pass\n",
        "    \"\"\"\n",
        "    #this code below can be used to load halted fitting results, but only weights (if model state is needed as well, fielpath1 shouldn't have an extension, and checkpoint below must have \"save_weights_only = False\")\n",
        "    filepath = \"act_weights.h5\"\n",
        "    new_model = load_model(f\"/content/drive/My Drive/V1_DNN/Proof_of_principal/L6/{filepath}\")\n",
        "    print(\"model loaded!\")\n",
        "\n",
        "\n",
        "    filepath1 = \"weights.{epoch:02d}-{loss:.6f}-minimal1.h5\"\n",
        "\n",
        "    checkpoint = ModelCheckpoint(f\"/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/{filepath1}\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "    callbacks_list = [checkpoint]\n",
        "    history = new_model.fit(x_train_counter, epochs=5000, steps_per_epoch = 5000, verbose=1, batch_size=64,\n",
        "                                shuffle=False, validation_data=x_test_counter, validation_steps=400, use_multiprocessing=False, callbacks=[callbacks_list])\n",
        " \n",
        "    \n",
        "    print(\"Done with training!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 128, 512)          30720     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 128, 256)          655616    \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 128, 128)          32896     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 16384)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 5, 128)            8454656   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 5, 128)            131584    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 5, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5, 128)            16512     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5, 100)            12900     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5, 100)            10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5, 100)            10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5, 100)            10100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5, 1)              101       \n",
            "=================================================================\n",
            "Total params: 9,496,869\n",
            "Trainable params: 9,496,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model loaded!\n",
            "Epoch 1/5000\n",
            "5000/5000 [==============================] - 843s 164ms/step - loss: 0.0012 - val_loss: 8.9859e-04\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.00119, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.01-0.001188-minimal1.h5\n",
            "Epoch 2/5000\n",
            "5000/5000 [==============================] - 814s 163ms/step - loss: 6.4827e-04 - val_loss: 7.4575e-04\n",
            "\n",
            "Epoch 00002: loss improved from 0.00119 to 0.00065, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.02-0.000648-minimal1.h5\n",
            "Epoch 3/5000\n",
            "5000/5000 [==============================] - 809s 162ms/step - loss: 6.1836e-04 - val_loss: 6.5451e-04\n",
            "\n",
            "Epoch 00003: loss improved from 0.00065 to 0.00062, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.03-0.000618-minimal1.h5\n",
            "Epoch 4/5000\n",
            "5000/5000 [==============================] - 813s 163ms/step - loss: 6.0475e-04 - val_loss: 4.9421e-04\n",
            "\n",
            "Epoch 00004: loss improved from 0.00062 to 0.00060, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.04-0.000605-minimal1.h5\n",
            "Epoch 5/5000\n",
            "5000/5000 [==============================] - 816s 163ms/step - loss: 5.9416e-04 - val_loss: 6.0338e-04\n",
            "\n",
            "Epoch 00005: loss improved from 0.00060 to 0.00059, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.05-0.000594-minimal1.h5\n",
            "Epoch 6/5000\n",
            "5000/5000 [==============================] - 822s 164ms/step - loss: 5.9602e-04 - val_loss: 6.9093e-04\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00059\n",
            "Epoch 7/5000\n",
            "5000/5000 [==============================] - 823s 165ms/step - loss: 5.8023e-04 - val_loss: 5.5911e-04\n",
            "\n",
            "Epoch 00007: loss improved from 0.00059 to 0.00058, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.07-0.000580-minimal1.h5\n",
            "Epoch 8/5000\n",
            "5000/5000 [==============================] - 997s 199ms/step - loss: 5.8552e-04 - val_loss: 4.7121e-04\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00058\n",
            "Epoch 9/5000\n",
            "5000/5000 [==============================] - 961s 192ms/step - loss: 5.7630e-04 - val_loss: 8.0674e-04\n",
            "\n",
            "Epoch 00009: loss improved from 0.00058 to 0.00058, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.09-0.000576-minimal1.h5\n",
            "Epoch 10/5000\n",
            "5000/5000 [==============================] - 956s 191ms/step - loss: 5.9156e-04 - val_loss: 5.3647e-04\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00058\n",
            "Epoch 11/5000\n",
            "5000/5000 [==============================] - 955s 191ms/step - loss: 5.7414e-04 - val_loss: 4.5019e-04\n",
            "\n",
            "Epoch 00011: loss improved from 0.00058 to 0.00057, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.11-0.000574-minimal1.h5\n",
            "Epoch 12/5000\n",
            "5000/5000 [==============================] - 960s 192ms/step - loss: 5.7546e-04 - val_loss: 6.9582e-04\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.00057\n",
            "Epoch 13/5000\n",
            "5000/5000 [==============================] - 897s 179ms/step - loss: 5.7070e-04 - val_loss: 4.5224e-04\n",
            "\n",
            "Epoch 00013: loss improved from 0.00057 to 0.00057, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.13-0.000571-minimal1.h5\n",
            "Epoch 14/5000\n",
            "5000/5000 [==============================] - 892s 178ms/step - loss: 5.6760e-04 - val_loss: 5.1780e-04\n",
            "\n",
            "Epoch 00014: loss improved from 0.00057 to 0.00057, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.14-0.000568-minimal1.h5\n",
            "Epoch 15/5000\n",
            "5000/5000 [==============================] - 895s 179ms/step - loss: 5.6832e-04 - val_loss: 5.5409e-04\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.00057\n",
            "Epoch 16/5000\n",
            "5000/5000 [==============================] - 894s 179ms/step - loss: 5.5756e-04 - val_loss: 4.0822e-04\n",
            "\n",
            "Epoch 00016: loss improved from 0.00057 to 0.00056, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.16-0.000558-minimal1.h5\n",
            "Epoch 17/5000\n",
            "5000/5000 [==============================] - 892s 178ms/step - loss: 5.6381e-04 - val_loss: 4.6358e-04\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.00056\n",
            "Epoch 18/5000\n",
            "5000/5000 [==============================] - 905s 181ms/step - loss: 5.5899e-04 - val_loss: 9.1275e-04\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.00056\n",
            "Epoch 19/5000\n",
            "5000/5000 [==============================] - 914s 183ms/step - loss: 5.7287e-04 - val_loss: 5.8535e-04\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.00056\n",
            "Epoch 20/5000\n",
            "5000/5000 [==============================] - 892s 178ms/step - loss: 5.5997e-04 - val_loss: 5.7450e-04\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.00056\n",
            "Epoch 21/5000\n",
            "5000/5000 [==============================] - 893s 179ms/step - loss: 5.4862e-04 - val_loss: 4.9699e-04\n",
            "\n",
            "Epoch 00021: loss improved from 0.00056 to 0.00055, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.21-0.000549-minimal1.h5\n",
            "Epoch 22/5000\n",
            "5000/5000 [==============================] - 900s 180ms/step - loss: 5.5211e-04 - val_loss: 7.2030e-04\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.00055\n",
            "Epoch 23/5000\n",
            "5000/5000 [==============================] - 899s 180ms/step - loss: 5.4583e-04 - val_loss: 4.4056e-04\n",
            "\n",
            "Epoch 00023: loss improved from 0.00055 to 0.00055, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.23-0.000546-minimal1.h5\n",
            "Epoch 24/5000\n",
            "5000/5000 [==============================] - 899s 180ms/step - loss: 5.5566e-04 - val_loss: 6.7290e-04\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.00055\n",
            "Epoch 25/5000\n",
            "5000/5000 [==============================] - 896s 179ms/step - loss: 5.5778e-04 - val_loss: 5.2292e-04\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.00055\n",
            "Epoch 26/5000\n",
            "5000/5000 [==============================] - 875s 175ms/step - loss: 5.3708e-04 - val_loss: 6.1388e-04\n",
            "\n",
            "Epoch 00026: loss improved from 0.00055 to 0.00054, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.26-0.000537-minimal1.h5\n",
            "Epoch 27/5000\n",
            "5000/5000 [==============================] - 874s 175ms/step - loss: 5.4901e-04 - val_loss: 7.0003e-04\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.00054\n",
            "Epoch 28/5000\n",
            "5000/5000 [==============================] - 872s 174ms/step - loss: 5.6178e-04 - val_loss: 5.2660e-04\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.00054\n",
            "Epoch 29/5000\n",
            "5000/5000 [==============================] - 874s 175ms/step - loss: 5.3543e-04 - val_loss: 6.0544e-04\n",
            "\n",
            "Epoch 00029: loss improved from 0.00054 to 0.00054, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.29-0.000535-minimal1.h5\n",
            "Epoch 30/5000\n",
            "5000/5000 [==============================] - 912s 182ms/step - loss: 5.3891e-04 - val_loss: 6.2333e-04\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.00054\n",
            "Epoch 31/5000\n",
            "5000/5000 [==============================] - 909s 182ms/step - loss: 5.3948e-04 - val_loss: 6.8912e-04\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.00054\n",
            "Epoch 32/5000\n",
            "5000/5000 [==============================] - 911s 182ms/step - loss: 5.3786e-04 - val_loss: 5.2337e-04\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.00054\n",
            "Epoch 33/5000\n",
            "5000/5000 [==============================] - 911s 182ms/step - loss: 5.4198e-04 - val_loss: 4.7848e-04\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.00054\n",
            "Epoch 34/5000\n",
            "5000/5000 [==============================] - 911s 182ms/step - loss: 5.4173e-04 - val_loss: 6.4579e-04\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.00054\n",
            "Epoch 35/5000\n",
            "5000/5000 [==============================] - 910s 182ms/step - loss: 5.3792e-04 - val_loss: 5.4385e-04\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.00054\n",
            "Epoch 36/5000\n",
            "5000/5000 [==============================] - 899s 180ms/step - loss: 5.4304e-04 - val_loss: 6.1590e-04\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.00054\n",
            "Epoch 37/5000\n",
            "5000/5000 [==============================] - 1016s 203ms/step - loss: 5.3829e-04 - val_loss: 4.3721e-04\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.00054\n",
            "Epoch 38/5000\n",
            "5000/5000 [==============================] - 1021s 204ms/step - loss: 5.3863e-04 - val_loss: 4.9106e-04\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.00054\n",
            "Epoch 39/5000\n",
            "5000/5000 [==============================] - 1040s 208ms/step - loss: 5.3774e-04 - val_loss: 6.7952e-04\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.00054\n",
            "Epoch 40/5000\n",
            "5000/5000 [==============================] - 1047s 209ms/step - loss: 5.3404e-04 - val_loss: 5.3717e-04\n",
            "\n",
            "Epoch 00040: loss improved from 0.00054 to 0.00053, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.40-0.000534-minimal1.h5\n",
            "Epoch 41/5000\n",
            "5000/5000 [==============================] - 1062s 212ms/step - loss: 5.3271e-04 - val_loss: 4.2942e-04\n",
            "\n",
            "Epoch 00041: loss improved from 0.00053 to 0.00053, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.41-0.000533-minimal1.h5\n",
            "Epoch 42/5000\n",
            "5000/5000 [==============================] - 1043s 209ms/step - loss: 5.4252e-04 - val_loss: 5.9129e-04\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.00053\n",
            "Epoch 43/5000\n",
            "5000/5000 [==============================] - 1036s 207ms/step - loss: 5.3216e-04 - val_loss: 4.8956e-04\n",
            "\n",
            "Epoch 00043: loss improved from 0.00053 to 0.00053, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.43-0.000532-minimal1.h5\n",
            "Epoch 44/5000\n",
            "5000/5000 [==============================] - 1040s 208ms/step - loss: 5.2756e-04 - val_loss: 4.1975e-04\n",
            "\n",
            "Epoch 00044: loss improved from 0.00053 to 0.00053, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.44-0.000528-minimal1.h5\n",
            "Epoch 45/5000\n",
            "5000/5000 [==============================] - 1037s 207ms/step - loss: 5.2846e-04 - val_loss: 5.4074e-04\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.00053\n",
            "Epoch 46/5000\n",
            "5000/5000 [==============================] - 1021s 204ms/step - loss: 5.3456e-04 - val_loss: 4.2632e-04\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.00053\n",
            "Epoch 47/5000\n",
            "5000/5000 [==============================] - 1028s 206ms/step - loss: 5.2722e-04 - val_loss: 4.3984e-04\n",
            "\n",
            "Epoch 00047: loss improved from 0.00053 to 0.00053, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.47-0.000527-minimal1.h5\n",
            "Epoch 48/5000\n",
            "5000/5000 [==============================] - 1030s 206ms/step - loss: 5.2429e-04 - val_loss: 4.9117e-04\n",
            "\n",
            "Epoch 00048: loss improved from 0.00053 to 0.00052, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.48-0.000524-minimal1.h5\n",
            "Epoch 49/5000\n",
            "5000/5000 [==============================] - 1026s 205ms/step - loss: 5.2362e-04 - val_loss: 6.1879e-04\n",
            "\n",
            "Epoch 00049: loss improved from 0.00052 to 0.00052, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.49-0.000524-minimal1.h5\n",
            "Epoch 50/5000\n",
            "5000/5000 [==============================] - 1031s 206ms/step - loss: 5.3100e-04 - val_loss: 5.5202e-04\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.00052\n",
            "Epoch 51/5000\n",
            "5000/5000 [==============================] - 1046s 209ms/step - loss: 5.1503e-04 - val_loss: 5.9803e-04\n",
            "\n",
            "Epoch 00051: loss improved from 0.00052 to 0.00052, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.51-0.000515-minimal1.h5\n",
            "Epoch 52/5000\n",
            "5000/5000 [==============================] - 1076s 215ms/step - loss: 5.2987e-04 - val_loss: 4.8917e-04\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.00052\n",
            "Epoch 53/5000\n",
            "5000/5000 [==============================] - 1068s 214ms/step - loss: 5.2101e-04 - val_loss: 4.7383e-04\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.00052\n",
            "Epoch 54/5000\n",
            "5000/5000 [==============================] - 1072s 214ms/step - loss: 5.2278e-04 - val_loss: 5.0117e-04\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.00052\n",
            "Epoch 55/5000\n",
            "5000/5000 [==============================] - 1101s 220ms/step - loss: 5.2195e-04 - val_loss: 4.5371e-04\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.00052\n",
            "Epoch 56/5000\n",
            "5000/5000 [==============================] - 1081s 216ms/step - loss: 5.3060e-04 - val_loss: 4.8456e-04\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.00052\n",
            "Epoch 57/5000\n",
            "5000/5000 [==============================] - 1073s 215ms/step - loss: 5.1906e-04 - val_loss: 6.4754e-04\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.00052\n",
            "Epoch 58/5000\n",
            "5000/5000 [==============================] - 1073s 215ms/step - loss: 5.2471e-04 - val_loss: 5.1865e-04\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.00052\n",
            "Epoch 59/5000\n",
            "5000/5000 [==============================] - 1210s 242ms/step - loss: 5.3564e-04 - val_loss: 7.1616e-04\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.00052\n",
            "Epoch 60/5000\n",
            "5000/5000 [==============================] - 1219s 244ms/step - loss: 5.3229e-04 - val_loss: 4.6891e-04\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.00052\n",
            "Epoch 61/5000\n",
            "5000/5000 [==============================] - 883s 177ms/step - loss: 5.2485e-04 - val_loss: 5.7207e-04\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.00052\n",
            "Epoch 62/5000\n",
            "5000/5000 [==============================] - 854s 171ms/step - loss: 5.2531e-04 - val_loss: 4.6872e-04\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.00052\n",
            "Epoch 63/5000\n",
            "5000/5000 [==============================] - 856s 171ms/step - loss: 5.2160e-04 - val_loss: 4.3794e-04\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.00052\n",
            "Epoch 64/5000\n",
            "5000/5000 [==============================] - 882s 177ms/step - loss: 5.2072e-04 - val_loss: 4.3156e-04\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.00052\n",
            "Epoch 65/5000\n",
            "5000/5000 [==============================] - 874s 175ms/step - loss: 5.1572e-04 - val_loss: 4.5105e-04\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.00052\n",
            "Epoch 66/5000\n",
            "5000/5000 [==============================] - 871s 174ms/step - loss: 5.2708e-04 - val_loss: 5.1565e-04\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.00052\n",
            "Epoch 67/5000\n",
            "5000/5000 [==============================] - 879s 176ms/step - loss: 5.1189e-04 - val_loss: 4.0594e-04\n",
            "\n",
            "Epoch 00067: loss improved from 0.00052 to 0.00051, saving model to /content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/weights.67-0.000512-minimal1.h5\n",
            "Epoch 68/5000\n",
            "5000/5000 [==============================] - 881s 176ms/step - loss: 5.1974e-04 - val_loss: 4.6333e-04\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.00051\n",
            "Epoch 69/5000\n",
            "5000/5000 [==============================] - 869s 174ms/step - loss: 5.1633e-04 - val_loss: 6.2799e-04\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.00051\n",
            "Epoch 70/5000\n",
            "5000/5000 [==============================] - 868s 174ms/step - loss: 5.1807e-04 - val_loss: 4.2344e-04\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.00051\n",
            "Epoch 71/5000\n",
            "5000/5000 [==============================] - 865s 173ms/step - loss: 5.1881e-04 - val_loss: 4.4815e-04\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.00051\n",
            "Epoch 72/5000\n",
            "5000/5000 [==============================] - 868s 174ms/step - loss: 5.1310e-04 - val_loss: 6.6160e-04\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.00051\n",
            "Epoch 73/5000\n",
            "5000/5000 [==============================] - 862s 172ms/step - loss: 5.1287e-04 - val_loss: 6.2529e-04\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.00051\n",
            "Epoch 74/5000\n",
            "5000/5000 [==============================] - 857s 171ms/step - loss: 5.2203e-04 - val_loss: 4.1299e-04\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.00051\n",
            "Epoch 75/5000\n",
            "5000/5000 [==============================] - 851s 170ms/step - loss: 5.2081e-04 - val_loss: 5.3784e-04\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.00051\n",
            "Epoch 76/5000\n",
            "5000/5000 [==============================] - 858s 172ms/step - loss: 5.2276e-04 - val_loss: 4.9051e-04\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.00051\n",
            "Epoch 77/5000\n",
            "2289/5000 [============>.................] - ETA: 7:01 - loss: 5.0665e-04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANN evaluaion**"
      ],
      "metadata": {
        "id": "26OcQD4hzih_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRQUIA5yuQYe"
      },
      "source": [
        "#these functions are just a coy of the ones before\n",
        "params = {\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 1,\n",
        "    \"lr\": 0.0010000,\n",
        "    \"time_steps\": 128,\n",
        "    \"rec_length\": 10000\n",
        "}\n",
        "\n",
        "\n",
        "TIME_STEPS = params[\"time_steps\"]\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "stime = time.time()\n",
        "steps_per_epoch = 2000\n",
        "\n",
        "def serve_CNN_LSTM(x):\n",
        "    return new_model(x, training=False)\n",
        "\n",
        "\n",
        "def print_time(text, stime):\n",
        "    seconds = (time.time()-stime)\n",
        "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")\n",
        "\n",
        "\n",
        "def trim_dataset(mat,batch_size):\n",
        "\n",
        "    no_of_rows_drop = mat.shape[0]%batch_size\n",
        "    if no_of_rows_drop > 0:\n",
        "        return mat[:-no_of_rows_drop]\n",
        "    else:\n",
        "        return mat\n",
        "\n",
        "def build_timeseries(mat, y_col_index):\n",
        "\n",
        "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
        "    dim_0 = mat.shape[0] - TIME_STEPS\n",
        "    dim_1 = mat.shape[1]\n",
        "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
        "    y = np.zeros((dim_0,1))\n",
        "    print(\"dim_0\",dim_0)\n",
        "    counter = 0\n",
        "    for i in range(dim_0):\n",
        "        #get rid of contaminated results (ie: end of trace - beginning of trace samples)\n",
        "        x[counter] = mat[counter:TIME_STEPS+counter]\n",
        "        y[counter] = mat[TIME_STEPS+counter, y_col_index:1]\n",
        "        counter += 1\n",
        "            \n",
        "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
        "    return x[:counter], y[:counter]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCiddrHtuQuX"
      },
      "source": [
        "#main function for dataset creation\n",
        "def create_dataset(data1):\n",
        "\n",
        "  df_train, df_test = train_test_split(data1, train_size=0.9, test_size=0.1, shuffle=False)\n",
        "  print(f\"Train size is: {len(df_train)}  -- Test size is: {len(df_test)}\")\n",
        "\n",
        "\n",
        "\n",
        "  x_train = np.asarray(df_train, dtype='float32')\n",
        "  x_test = np.asarray(df_test, dtype='float32')\n",
        "\n",
        "  x_t, y_t = build_timeseries(x_train, 0)\n",
        "  \n",
        "  del x_train\n",
        "  del df_train\n",
        "\n",
        "\n",
        "  x_t = trim_dataset(x_t, BATCH_SIZE)\n",
        "  y_t = trim_dataset(y_t, BATCH_SIZE)\n",
        "  print(\"Batch trimmed size\",x_t.shape, y_t.shape)\n",
        "\n",
        "  #this is a must have, otherwise the model thinks that the 3 labels are sequential in time\n",
        "  y_t = np.reshape(y_t, [y_t.shape[0], 1, 1])\n",
        "\n",
        "  x_t = np.array(x_t, dtype='float32')\n",
        "  y_t = np.array(y_t, dtype='float32')\n",
        "\n",
        "  x_temp, y_temp = build_timeseries(x_test, 0)\n",
        "\n",
        "  y_temp = np.reshape(y_temp, [y_temp.shape[0], 1, 1])\n",
        "\n",
        "  x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
        "  y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
        "\n",
        "  print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n",
        "\n",
        "  x_val = np.array(x_val, dtype='float16')\n",
        "  y_val = np.array(y_val, dtype='float16')\n",
        "\n",
        "  del y_temp\n",
        "  del x_test\n",
        "  del df_test\n",
        "\n",
        "  return x_t, y_t, x_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKWhtsx2uSIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6dd385c-433b-4e87-bcca-19f5e112567d"
      },
      "source": [
        "#we need to load an additional dataset for evaluation\n",
        "data = pd.read_csv('/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/vmi3_2.txt', sep=\" \", header=None, dtype='float32', nrows=300000)\n",
        "\n",
        "data.drop([59], axis = 1, inplace=True)\n",
        "data.columns = [str(data.columns[i]) for i in range(59)]\n",
        "\n",
        "for place, val in enumerate(data[\"0\"]):\n",
        "    if val > -20:\n",
        "        data[\"0\"][place] = -20\n",
        "\n",
        "data[\"0\"] = data[\"0\"]+150   #somatic Vm\n",
        "\n",
        "data[\"0\"] = (data[\"0\"]-48.44300079345703)/130.0\n",
        "data[\"0\"] = data[\"0\"]/0.6273615323580228\n",
        "\n",
        "#get the size of a 0.74 mV, 0.63 mV, 0.75 mV and 0.23 mV input\n",
        "\n",
        "print(((80.74-48.44300079345703)/130.0)/0.6273615323580228-((80-48.44300079345703)/130.0)/0.6273615323580228)\n",
        "print(((80.63-48.44300079345703)/130.0)/0.6273615323580228-((80-48.44300079345703)/130.0)/0.6273615323580228)\n",
        "print(((80.75-48.44300079345703)/130.0)/0.6273615323580228-((80-48.44300079345703)/130.0)/0.6273615323580228)\n",
        "print(((80.23-48.44300079345703)/130.0)/0.6273615323580228-((80-48.44300079345703)/130.0)/0.6273615323580228)\n",
        "x_all = np.asarray(data, dtype=\"float32\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009073408869862243\n",
            "0.007724658902720505\n",
            "0.00919602250323881\n",
            "0.002820113567659932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dWX8bGSuZfT",
        "outputId": "f7d5c99b-0605-495e-eb82-6c0fc13142f0"
      },
      "source": [
        "x_t, y_t, x_val, y_val = create_dataset(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size is: 270000  -- Test size is: 30000\n",
            "dim_0 269872\n",
            "length of time-series i/o (269872, 128, 59) (269872, 1)\n",
            "Batch trimmed size (269824, 128, 59) (269824, 1)\n",
            "dim_0 29872\n",
            "length of time-series i/o (29872, 128, 59) (29872, 1)\n",
            "Test size (14912, 128, 59) (14912, 1, 1) (14912, 128, 59) (14912, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del data"
      ],
      "metadata": {
        "id": "SkdScjsUA8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlRg0s2Mud-7"
      },
      "source": [
        "#loading final ANN\n",
        "filepath = \"weights.67-0.000512-minimal1.h5\"\n",
        "\n",
        "# load the model\n",
        "new_model = load_model(f\"/content/drive/My Drive/V1_DNN/Proof_of_principal/L2_3/{filepath}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thkr14YKunnV"
      },
      "source": [
        "#checking ANN fit\n",
        "pred = np.mean(serve_CNN_LSTM(x_t[:10000]), axis=1)\n",
        "\n",
        "GT = []\n",
        "for val in x_t[:10001]:\n",
        "    GT.append(val[-1][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "4xsHHNQQuqig",
        "outputId": "c3a3249b-2972-43f7-8b76-142a42fce5cb"
      },
      "source": [
        "plt.plot(np.asarray(pred), GT[1:], 'go')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff3f8200a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "wBrZPIrXuufq",
        "outputId": "39b392d1-0495-423d-a50d-0b18e584e282"
      },
      "source": [
        "plt.plot(pred[:500])\n",
        "plt.plot(GT[1:501])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoaS9412zjaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d29f8e2-2d04-4ef2-be77-8a484f959ab7"
      },
      "source": [
        "#this function runs 50 ANNs on 50 different traces, calculates variance explained, precision and recall, and gives an error value based on spike timing\n",
        "#as the dataset is non normally distributed, there will be slight biases in the prediction, which can be corrected for\n",
        "#the argument of this function takes a list of two vlues, which are used to correct predictions linearly (y = mx+b  --> corrected_prediction = arg1[0]*prediction + arg1[1])\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "gc.collect()\n",
        "\n",
        "def go_batch(arg1):\n",
        "    \"\"\"\n",
        "    global GT_spikes\n",
        "    global res_spikes\n",
        "    global GT\n",
        "    global starting_points\n",
        "    \"\"\"\n",
        "    data1 = np.asarray(data, dtype=\"float32\")\n",
        "\n",
        "    array_count = 50\n",
        "    starting_points = []\n",
        "    \n",
        "\n",
        "    #gather traces with at least a couple of spikes\n",
        "    for i in range(int(data.shape[0]/1000)-1):\n",
        "        temp1 = np.sort(np.asarray(data['0'][(i*1000) + 200 :(i*1000) + 1200]))\n",
        "        if temp1[-5] > 0.8 and (i+1)%10 > 0:\n",
        "            starting_points.append((i*1000) + 200)\n",
        "    \n",
        "    #reduce the number of traces to equal array_count\n",
        "    starting_points = starting_points[:50]\n",
        "\n",
        "\n",
        "    #initialize the first starting point, the rest will be added by concatenation\n",
        "    inputs = np.reshape(data1[starting_points[0]:starting_points[0]+128], [1,128,59])\n",
        "\n",
        "    MAEs = []\n",
        "    for i in range(array_count-1):\n",
        "        inputs = np.concatenate((inputs, np.reshape(data1[starting_points[i+1]:starting_points[i+1]+128], [1,128,59])))\n",
        "\n",
        "\n",
        "    res = [[] for i in range(array_count)]\n",
        "    GT = [[] for i in range(array_count)]\n",
        "\n",
        "    #run the simpulation for 1000 datapoints (500 ms)\n",
        "    for i in range(1000):\n",
        "        pred = np.mean(serve_CNN_LSTM(inputs), axis=1)\n",
        "        for k in range(array_count):\n",
        "            res[k].append(pred[k][0])\n",
        "            temp = inputs[k][1:]\n",
        "            mock_row = data1[starting_points[k]+128+i]\n",
        "            GT[k].append(mock_row[0])\n",
        "            mock_row[0] = pred[k][0]* arg1[0] + arg1[1]\n",
        "            temp = np.concatenate((temp,np.reshape(mock_row, [1,59])))\n",
        "            inputs[k] = temp\n",
        "    for i in range(array_count):\n",
        "        MAEs.append(mean_absolute_error(res[i], GT[i]))\n",
        "    \n",
        "    #get var explained\n",
        "    print(np.asarray(res).shape)\n",
        "    print(np.asarray(y_t).shape)\n",
        "    for i in range(50):\n",
        "        vm_out = np.transpose(res[i])\n",
        "        vm_GT = np.transpose(np.asarray(y_t[starting_points[i]:starting_points[i]+1000,0,0]))\n",
        "        #plt.plot(vm_out)\n",
        "        #plt.plot(vm_GT)\n",
        "        #plt.show()\n",
        "        #mse = np.mean((vm_out-vm_GT)**2)\n",
        "        temp_arr = []\n",
        "        for k in range(vm_out.shape[0]):\n",
        "            if vm_out[k] < 0.6 and vm_GT[k] < 0.6:\n",
        "                temp_arr.append((vm_out[k]-vm_GT[k])**2)\n",
        "        mse = np.mean(np.asarray(temp_arr))\n",
        "        #var = np.var(vm_GT)\n",
        "        temp_GT = list(filter(lambda x: x < 0.6, list(vm_GT)))\n",
        "        var = np.var(np.asarray(temp_GT))\n",
        "        var_explained = 1-(mse/var)\n",
        "        print(var_explained)\n",
        "\n",
        "\n",
        "    #get spikes from GT and predictions\n",
        "    GT_spikes = [[] for i in range(array_count)]\n",
        "    res_spikes = [[] for i in range(array_count)]\n",
        "    for i in range(array_count):\n",
        "        ms = 0\n",
        "        while ms < len(GT[i]):\n",
        "            if GT[i][ms] > 0.8:\n",
        "                GT_spikes[i].append(ms)\n",
        "                ms += 6\n",
        "            ms += 1\n",
        "        ms = 0\n",
        "        while ms < len(res[i]):\n",
        "            if res[i][ms] > 0.8:\n",
        "                res_spikes[i].append(ms)\n",
        "                ms += 6\n",
        "            ms += 1\n",
        "\n",
        "    #compare spike timings\n",
        "    TP = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    for i in range(array_count):\n",
        "        TP_temp = 0\n",
        "        GT_temp = np.sort(GT_spikes[i])\n",
        "    \n",
        "        #precision = TP / (TP+FP)\n",
        "        #recall = TP / (TP+FN)\n",
        "        if len(res_spikes[i]) == 0:\n",
        "            FN += len(GT_temp)\n",
        "        else:\n",
        "            for item in GT_temp:\n",
        "                res_temp = [x-item for x in res_spikes[i]]\n",
        "                res_temp = list(map(abs, res_temp))\n",
        "                if np.sort(res_temp)[0] < 10:\n",
        "                    TP_temp += 1\n",
        "            TP += TP_temp\n",
        "            FN += (len(GT_temp) - TP_temp)\n",
        "            FP += (len(res_temp) - TP_temp)\n",
        "        print(TP_temp,(len(GT_temp) - TP_temp),(len(res_temp) - TP_temp))\n",
        "            #print(GT_temp, res_spikes[i], TP_temp, (len(GT_temp) - TP_temp), (len(res_temp) - TP_temp))\n",
        "    #print(TP, FP, FN)\n",
        "\n",
        "    if TP == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        distance = 0\n",
        "    else:\n",
        "        precision = TP / (TP+FP)\n",
        "        recall = TP / (TP+FN)\n",
        "        distance = abs(precision-recall)\n",
        "\n",
        "    print(f\" {arg1[0]} {arg1[1]} {precision} {recall} {distance} {precision+recall-distance} {3-(precision+recall-distance)}\")\n",
        "    return 3-(precision+recall-distance)\n",
        "\n",
        "go_batch([0.9999653186890414, 8.220763382875164e-05])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 1000)\n",
            "(269824, 1, 1)\n",
            "0.8085910807868639\n",
            "0.6656371382126185\n",
            "0.5731066919254775\n",
            "0.7394539109821108\n",
            "0.14557768849186292\n",
            "0.6783047264373904\n",
            "0.5017443093333573\n",
            "0.7723531934914205\n",
            "0.6349789825965368\n",
            "0.5042742613244504\n",
            "0.5365027914133877\n",
            "0.6658362180290344\n",
            "0.6695142341975556\n",
            "0.8137341868521789\n",
            "0.728417063630401\n",
            "0.7257727306857906\n",
            "0.7602885670919701\n",
            "0.8089938272517283\n",
            "0.7438752024039548\n",
            "0.37387025480635083\n",
            "0.7050268323444291\n",
            "0.7921093373611302\n",
            "0.7744112215408117\n",
            "0.5184683900549231\n",
            "0.5382441288631001\n",
            "0.870051518496961\n",
            "0.6470299234905429\n",
            "0.8134936189417177\n",
            "0.7626078712427223\n",
            "0.7035446943625642\n",
            "0.7802059428342834\n",
            "0.5323855847692947\n",
            "0.8992375600983089\n",
            "0.6430351024946028\n",
            "0.5934785936253779\n",
            "0.6477130912432332\n",
            "0.8684420176156098\n",
            "0.786687341399214\n",
            "0.7132055326842675\n",
            "0.5696786679328709\n",
            "0.6117732705298672\n",
            "0.8698786514323411\n",
            "0.45424870328062716\n",
            "0.5767386883990686\n",
            "0.8321360757869832\n",
            "0.4682392109568153\n",
            "0.5347699303198463\n",
            "0.7438690258925069\n",
            "0.7335561194640303\n",
            "0.6870210522674158\n",
            "2 0 0\n",
            "1 0 0\n",
            "1 0 0\n",
            "3 0 0\n",
            "1 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "4 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "1 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "1 0 0\n",
            "2 0 0\n",
            "4 0 0\n",
            "5 0 0\n",
            "2 0 0\n",
            "4 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "4 0 0\n",
            "3 0 0\n",
            "4 0 0\n",
            "2 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "4 0 0\n",
            "2 0 0\n",
            "3 0 0\n",
            "2 0 0\n",
            "4 0 0\n",
            " 0.9999653186890414 8.220763382875164e-05 1.0 1.0 0.0 2.0 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxcH4NIAOCg"
      },
      "source": [
        "#minimization procedure of spike timing\n",
        "from scipy.optimize import minimize\n",
        "x0 = []\n",
        "x0.append(1.0)\n",
        "x0.append(0.0)\n",
        "\n",
        "res = minimize(go_batch, x0, method='nelder-mead', tol=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaTTHT6vXac7"
      },
      "source": [
        "#to evaluate everything, validation has to be rerun on a different dataset (same as for validation), and go_batch function calculates everything "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}